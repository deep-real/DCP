{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils.factory import create_model_and_transforms, get_tokenizer\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import torch.nn as nn\n",
    "from load import *\n",
    "import random\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(image_features, text_features, bs = 1000):\n",
    "    # compute similarity\n",
    "    max_pairs = image_features.shape[0]\n",
    "    similarity_scores = torch.zeros(max_pairs, max_pairs)\n",
    "    for v in range(0, max_pairs, bs):\n",
    "        for t in range(0, max_pairs, bs):\n",
    "            print('Processing Visual '+str(v)+' Text '+str(t), end='\\r')\n",
    "            batch_visual_emb = image_features[v:v+bs]\n",
    "            batch_caption_emb = text_features[t:t+bs]\n",
    "\n",
    "            batch_visual_emb = batch_visual_emb.to(torch.float32)\n",
    "            batch_caption_emb = batch_caption_emb.to(torch.float32)\n",
    "\n",
    "            logits = batch_visual_emb @ batch_caption_emb.t()\n",
    "            similarity_scores[v:v+bs,t:t+bs] = logits\n",
    "\n",
    "    print('Done similarity')\n",
    "    return similarity_scores\n",
    "\n",
    "def compute_retrieval(a2b_sims, return_ranks=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        a2b_sims: Result of computing similarity between two sets of embeddings (emb1 @ emb2.T)\n",
    "            with shape (num_datapoints, num_datapoints).\n",
    "\n",
    "    Returns:\n",
    "        Retrieval metrics for that similarity.\n",
    "    \"\"\"\n",
    "    npts = a2b_sims.shape[0]\n",
    "    ranks = np.zeros(npts)\n",
    "    top1 = np.zeros(npts)\n",
    "    # loop source embedding indices\n",
    "    for index in range(npts):\n",
    "        # get order of similarities to target embeddings\n",
    "        inds = np.argsort(a2b_sims[index])[::-1]\n",
    "        # find where the correct embedding is ranked\n",
    "        where = np.where(inds == index)\n",
    "        rank = where[0][0]\n",
    "        ranks[index] = rank\n",
    "        # save the top1 result as well\n",
    "        top1[index] = inds[0]\n",
    "\n",
    "    # Compute metrics\n",
    "    r1 = 100.0 * len(np.where(ranks < 1)[0]) / len(ranks)\n",
    "    r5 = 100.0 * len(np.where(ranks < 5)[0]) / len(ranks)\n",
    "    r10 = 100.0 * len(np.where(ranks < 10)[0]) / len(ranks)\n",
    "    r50 = 100.0 * len(np.where(ranks < 50)[0]) / len(ranks)\n",
    "    medr = np.floor(np.median(ranks)) + 1\n",
    "    meanr = ranks.mean() + 1\n",
    "\n",
    "    report_dict = {\"r1\": r1, \"r5\": r5, \"r10\": r10, \"r50\": r50, \"medr\": medr, \"meanr\": meanr, \"sum\": r1 + r5 + r10}\n",
    "\n",
    "    if return_ranks:\n",
    "        return report_dict, (ranks, top1)\n",
    "    else:\n",
    "        return report_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-B-32\"\n",
    "\n",
    "# pretrained = \"openai\"\n",
    "pretrained = \"./DCP-ViT-B-32.pt\"\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = create_model_and_transforms(\n",
    "    model_name, pretrained=pretrained\n",
    ")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\",\n",
    ")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model.visual.transformer.resblocks))\n",
    "\n",
    "# prs = hook_prs_logger(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_classifier(classnames, templates):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname) for template in templates] #format with class\n",
    "            # texts = clip.tokenize(texts).cuda() #tokenize\n",
    "            texts = tokenizer(texts).cuda() #tokenize\n",
    "            class_embeddings = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cifar10_val = CIFAR10(root=CIFAR10_DIR, train=False, transform=preprocess)\n",
    "dataloader_cifar10_val = DataLoader(dataset_cifar10_val, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = zeroshot_classifier(label_to_classname_cifar10, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_cifar10_val)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cifar100_val = CIFAR100(root=CIFAR100_DIR, train=False, transform=preprocess)\n",
    "dataloader_cifar100_val = DataLoader(dataset_cifar100_val, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = zeroshot_classifier(label_to_classname_cifar100, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_cifar100_val)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cub_val = CUBDataset(CUB_DIR, train=False, transform=preprocess)\n",
    "dataloader_cub_val = DataLoader(dataset_cub_val, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = zeroshot_classifier(label_to_classname_cub, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_cub_val)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_caltech101 = torchvision.datasets.Caltech101(root=CALTECH101_DIR, transform=preprocess)\n",
    "dataloader_caltech101 = DataLoader(dataset_caltech101, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = zeroshot_classifier(label_to_classname_caltech101, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_caltech101)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_oxfordpets_tst = torchvision.datasets.OxfordIIITPet(root=OXFORDPET_DIR, transform=preprocess, split='test')\n",
    "dataloader_oxfordpets_tst = DataLoader(dataset_oxfordpets_tst, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = zeroshot_classifier(label_to_classname_oxfordpets, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_oxfordpets_tst)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_food101_tst = torchvision.datasets.Food101(root=FOOD101_DIR, transform=preprocess, split='test')\n",
    "dataloader_food101_tst = DataLoader(dataset_food101_tst, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = zeroshot_classifier(label_to_classname_food101, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_food101_tst)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sun397 = torchvision.datasets.SUN397(root=SUN397_DIR, transform=preprocess)\n",
    "dataset_sun397_trn, dataset_sun397_tst = torch.utils.data.random_split(dataset_sun397, [100000, 8754])\n",
    "dataloader_sun397_tst = DataLoader(dataset_sun397_tst, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = zeroshot_classifier(label_to_classname_sun397, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_sun397_tst)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stanfordcars_trn, dataset_stanfordcars_tst = torch.utils.data.random_split(dataset_stanfordcars, [6000, 2144])\n",
    "dataloader_stanfordcars_tst = DataLoader(dataset_stanfordcars_tst, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeroshot_weights = zeroshot_classifier(label_to_classname_stanfordcars, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_stanfordcars_tst)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_dtd = DataLoader(dataset_dtd, batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "zeroshot_weights = zeroshot_classifier(label_to_classname_dtd, imagenet_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(dataloader_dtd)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images, attn_method=\"direct\")\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, acc5 = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        top5 += acc5\n",
    "        n += images.size(0)\n",
    "\n",
    "top1 = (top1 / n) * 100\n",
    "top5 = (top5 / n) * 100 \n",
    "\n",
    "print(f\"Top-1 accuracy: {top1:.2f}\")\n",
    "print(f\"Top-5 accuracy: {top5:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
