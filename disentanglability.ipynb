{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os.path\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from utils.factory import create_model_and_transforms, get_tokenizer\n",
    "from utils.binary_waterbirds import BinaryWaterbirds\n",
    "from prs_hook import hook_prs_logger\n",
    "from torchvision.datasets import CIFAR100, CIFAR10, ImageNet, ImageFolder\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "import torch.nn as nn\n",
    "from load import *\n",
    "import random\n",
    "from torch import optim\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"ViT-B-32\"\n",
    "\n",
    "# pretrained = \"openai\"\n",
    "pretrained = \"./DCP-ViT-B-32.pt\"\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, preprocess = create_model_and_transforms(\n",
    "    model_name, pretrained=pretrained\n",
    ")\n",
    "model.to(device)\n",
    "# model.eval()\n",
    "model.eval()\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\n",
    "    \"Model parameters:\",\n",
    "    f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\",\n",
    ")\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"Len of res:\", len(model.visual.transformer.resblocks))\n",
    "\n",
    "prs = hook_prs_logger(model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCosineOrthogonalLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BatchCosineOrthogonalLoss, self).__init__()\n",
    "\n",
    "    def forward(self, heatmaps):\n",
    "        b, n, _ = heatmaps.shape  # Assume heatmaps are [batch_size, n, features]\n",
    "\n",
    "        # Normalize the heatmaps along the last dimension\n",
    "        norm = torch.norm(heatmaps, p=2, dim=-1, keepdim=True)\n",
    "        normalized_heatmaps = heatmaps / norm\n",
    "\n",
    "        # Compute the cosine similarities using batched matrix multiplication\n",
    "        cosine_similarities = torch.bmm(normalized_heatmaps, normalized_heatmaps.transpose(1, 2))\n",
    "        \n",
    "        # Zero out the diagonal (self-cosine similarities)\n",
    "        mask = torch.eye(n, device=cosine_similarities.device).bool()\n",
    "        cosine_similarities.masked_fill_(mask.unsqueeze(0), 0)\n",
    "\n",
    "        # Square the off-diagonal elements\n",
    "        loss_values = cosine_similarities ** 2\n",
    "\n",
    "        # Sum the squared values and normalize by the total number of off-diagonal elements in the batch\n",
    "        loss = loss_values.sum() / (b * n * (n - 1))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_orth = BatchCosineOrthogonalLoss()\n",
    "tokenizer = get_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['descriptor_fname'] = './descriptors/my_cifar10.json'\n",
    "label_to_classname = label_to_classname_cifar10\n",
    "gpt_descriptions, unmodify_dict = load_gpt_descriptions(hparams, label_to_classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cifar10_val = CIFAR10(root=CIFAR10_DIR, train=False, transform=preprocess)\n",
    "dataloader_val = DataLoader(dataset_cifar10_val, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for batch_number, batch in enumerate(tqdm(dataloader_val)):\n",
    "        images, labels = batch\n",
    "\n",
    "        texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "        tokenized_concepts_list = []\n",
    "        for i in range(len(texts)):\n",
    "            concepts = gpt_descriptions[texts[i]][:5]\n",
    "            tokenized_concepts = tokenizer(concepts)\n",
    "            tokenized_concepts_list.append(tokenized_concepts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        prs.reinit()\n",
    "        representation = model.encode_image(\n",
    "            images, attn_method=\"head\", normalize=False\n",
    "        )\n",
    "        attentions = prs.finalize(representation)\n",
    "\n",
    "        tokenized_concepts_list = torch.stack(tokenized_concepts_list).reshape(-1, 77)\n",
    "        node_text_embeddings = model.encode_text(tokenized_concepts_list.to(device))\n",
    "        node_text_embeddings = node_text_embeddings.reshape(len(images), -1, 512)\n",
    "        \n",
    "        attentions_maps = []\n",
    "        for i in range(len(attentions)):\n",
    "            attentions_map = attentions[i, :, 1:, :].sum(axis=(0, 2)) @ node_text_embeddings[i].T\n",
    "            attentions_maps.append(attentions_map.permute(1, 0))\n",
    "        attentions_maps = torch.stack(attentions_maps)\n",
    "\n",
    "        orth_loss = loss_orth(attentions_maps)\n",
    "\n",
    "        total_loss += orth_loss.item()\n",
    "\n",
    "avg_orth = total_loss / batch_number\n",
    "1 - avg_orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['descriptor_fname'] = './descriptors/my_cifar100.json'\n",
    "label_to_classname = label_to_classname_cifar100\n",
    "gpt_descriptions, unmodify_dict = load_gpt_descriptions(hparams, label_to_classname)\n",
    "\n",
    "dataset_cifar100_val = CIFAR100(root=CIFAR100_DIR, train=False, transform=preprocess)\n",
    "dataloader_val = DataLoader(dataset_cifar100_val, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for batch_number, batch in enumerate(tqdm(dataloader_val)):\n",
    "        images, labels = batch\n",
    "\n",
    "        texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "        tokenized_concepts_list = []\n",
    "        for i in range(len(texts)):\n",
    "            concepts = gpt_descriptions[texts[i]][:4]\n",
    "            tokenized_concepts = tokenizer(concepts)\n",
    "            tokenized_concepts_list.append(tokenized_concepts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        prs.reinit()\n",
    "        representation = model.encode_image(\n",
    "            images, attn_method=\"head\", normalize=False\n",
    "        )\n",
    "        attentions = prs.finalize(representation)\n",
    "\n",
    "        tokenized_concepts_list = torch.stack(tokenized_concepts_list).reshape(-1, 77)\n",
    "        node_text_embeddings = model.encode_text(tokenized_concepts_list.to(device))\n",
    "        node_text_embeddings = node_text_embeddings.reshape(len(images), -1, 512)\n",
    "        \n",
    "        attentions_maps = []\n",
    "        for i in range(len(attentions)):\n",
    "            attentions_map = attentions[i, :, 1:, :].sum(axis=(0, 2)) @ node_text_embeddings[i].T\n",
    "            attentions_maps.append(attentions_map.permute(1, 0))\n",
    "        attentions_maps = torch.stack(attentions_maps)\n",
    "\n",
    "        orth_loss = loss_orth(attentions_maps)\n",
    "\n",
    "        total_loss += orth_loss.item()\n",
    "\n",
    "avg_orth = total_loss / batch_number\n",
    "1 - avg_orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['descriptor_fname'] = './descriptors/my_cub.json'\n",
    "label_to_classname = label_to_classname_cub\n",
    "gpt_descriptions, unmodify_dict = load_gpt_descriptions(hparams, label_to_classname)\n",
    "\n",
    "dataset_cub_val = CUBDataset(CUB_DIR, train=False, transform=preprocess)\n",
    "dataloader_val = DataLoader(dataset_cub_val, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for batch_number, batch in enumerate(tqdm(dataloader_val)):\n",
    "        images, labels = batch\n",
    "\n",
    "        texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "        tokenized_concepts_list = []\n",
    "        for i in range(len(texts)):\n",
    "            concepts = gpt_descriptions[texts[i]][:5]\n",
    "            tokenized_concepts = tokenizer(concepts)\n",
    "            tokenized_concepts_list.append(tokenized_concepts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        prs.reinit()\n",
    "        representation = model.encode_image(\n",
    "            images, attn_method=\"head\", normalize=False\n",
    "        )\n",
    "        attentions = prs.finalize(representation)\n",
    "\n",
    "        tokenized_concepts_list = torch.stack(tokenized_concepts_list).reshape(-1, 77)\n",
    "        node_text_embeddings = model.encode_text(tokenized_concepts_list.to(device))\n",
    "        node_text_embeddings = node_text_embeddings.reshape(len(images), -1, 512)\n",
    "        \n",
    "        attentions_maps = []\n",
    "        for i in range(len(attentions)):\n",
    "            attentions_map = attentions[i, :, 1:, :].sum(axis=(0, 2)) @ node_text_embeddings[i].T\n",
    "            attentions_maps.append(attentions_map.permute(1, 0))\n",
    "        attentions_maps = torch.stack(attentions_maps)\n",
    "\n",
    "        orth_loss = loss_orth(attentions_maps)\n",
    "\n",
    "        total_loss += orth_loss.item()\n",
    "\n",
    "avg_orth = total_loss / batch_number\n",
    "1 - avg_orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['descriptor_fname'] = './descriptors/my_caltech101.json'\n",
    "label_to_classname = label_to_classname_caltech101\n",
    "gpt_descriptions, unmodify_dict = load_gpt_descriptions(hparams, label_to_classname)\n",
    "\n",
    "dataset_caltech101 = torchvision.datasets.Caltech101(root=CALTECH101_DIR, transform=preprocess)\n",
    "dataloader_val = DataLoader(dataset_caltech101, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for batch_number, batch in enumerate(tqdm(dataloader_val)):\n",
    "        images, labels = batch\n",
    "\n",
    "        texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "        tokenized_concepts_list = []\n",
    "        for i in range(len(texts)):\n",
    "            concepts = gpt_descriptions[texts[i]][:5]\n",
    "            tokenized_concepts = tokenizer(concepts)\n",
    "            tokenized_concepts_list.append(tokenized_concepts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        prs.reinit()\n",
    "        representation = model.encode_image(\n",
    "            images, attn_method=\"head\", normalize=False\n",
    "        )\n",
    "        attentions = prs.finalize(representation)\n",
    "\n",
    "        tokenized_concepts_list = torch.stack(tokenized_concepts_list).reshape(-1, 77)\n",
    "        node_text_embeddings = model.encode_text(tokenized_concepts_list.to(device))\n",
    "        node_text_embeddings = node_text_embeddings.reshape(len(images), -1, 512)\n",
    "        \n",
    "        attentions_maps = []\n",
    "        for i in range(len(attentions)):\n",
    "            attentions_map = attentions[i, :, 1:, :].sum(axis=(0, 2)) @ node_text_embeddings[i].T\n",
    "            attentions_maps.append(attentions_map.permute(1, 0))\n",
    "        attentions_maps = torch.stack(attentions_maps)\n",
    "\n",
    "        orth_loss = loss_orth(attentions_maps)\n",
    "\n",
    "        total_loss += orth_loss.item()\n",
    "\n",
    "avg_orth = total_loss / batch_number\n",
    "1 - avg_orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['descriptor_fname'] = './descriptors/my_oxfordpet.json'\n",
    "label_to_classname = label_to_classname_oxfordpets\n",
    "gpt_descriptions, unmodify_dict = load_gpt_descriptions(hparams, label_to_classname)\n",
    "\n",
    "dataset_oxfordpets_tst = torchvision.datasets.OxfordIIITPet(root=OXFORDPET_DIR, transform=preprocess, split='test')\n",
    "dataloader_val = DataLoader(dataset_oxfordpets_tst, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for batch_number, batch in enumerate(tqdm(dataloader_val)):\n",
    "        images, labels = batch\n",
    "\n",
    "        texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "        tokenized_concepts_list = []\n",
    "        for i in range(len(texts)):\n",
    "            concepts = gpt_descriptions[texts[i]][:5]\n",
    "            tokenized_concepts = tokenizer(concepts)\n",
    "            tokenized_concepts_list.append(tokenized_concepts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        prs.reinit()\n",
    "        representation = model.encode_image(\n",
    "            images, attn_method=\"head\", normalize=False\n",
    "        )\n",
    "        attentions = prs.finalize(representation)\n",
    "\n",
    "        tokenized_concepts_list = torch.stack(tokenized_concepts_list).reshape(-1, 77)\n",
    "        node_text_embeddings = model.encode_text(tokenized_concepts_list.to(device))\n",
    "        node_text_embeddings = node_text_embeddings.reshape(len(images), -1, 512)\n",
    "        \n",
    "        attentions_maps = []\n",
    "        for i in range(len(attentions)):\n",
    "            attentions_map = attentions[i, :, 1:, :].sum(axis=(0, 2)) @ node_text_embeddings[i].T\n",
    "            attentions_maps.append(attentions_map.permute(1, 0))\n",
    "        attentions_maps = torch.stack(attentions_maps)\n",
    "\n",
    "        orth_loss = loss_orth(attentions_maps)\n",
    "\n",
    "        total_loss += orth_loss.item()\n",
    "\n",
    "avg_orth = total_loss / batch_number\n",
    "1 - avg_orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['descriptor_fname'] = './descriptors/my_food101.json'\n",
    "label_to_classname = label_to_classname_food101\n",
    "gpt_descriptions, unmodify_dict = load_gpt_descriptions(hparams, label_to_classname)\n",
    "\n",
    "dataset_food101_tst = torchvision.datasets.Food101(root=FOOD101_DIR, transform=preprocess, split='test')\n",
    "dataloader_val = DataLoader(dataset_food101_tst, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for batch_number, batch in enumerate(tqdm(dataloader_val)):\n",
    "        images, labels = batch\n",
    "\n",
    "        texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "        tokenized_concepts_list = []\n",
    "        for i in range(len(texts)):\n",
    "            concepts = gpt_descriptions[texts[i]][:5]\n",
    "            tokenized_concepts = tokenizer(concepts)\n",
    "            tokenized_concepts_list.append(tokenized_concepts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        prs.reinit()\n",
    "        representation = model.encode_image(\n",
    "            images, attn_method=\"head\", normalize=False\n",
    "        )\n",
    "        attentions = prs.finalize(representation)\n",
    "\n",
    "        tokenized_concepts_list = torch.stack(tokenized_concepts_list).reshape(-1, 77)\n",
    "        node_text_embeddings = model.encode_text(tokenized_concepts_list.to(device))\n",
    "        node_text_embeddings = node_text_embeddings.reshape(len(images), -1, 512)\n",
    "        \n",
    "        attentions_maps = []\n",
    "        for i in range(len(attentions)):\n",
    "            attentions_map = attentions[i, :, 1:, :].sum(axis=(0, 2)) @ node_text_embeddings[i].T\n",
    "            attentions_maps.append(attentions_map.permute(1, 0))\n",
    "        attentions_maps = torch.stack(attentions_maps)\n",
    "\n",
    "        orth_loss = loss_orth(attentions_maps)\n",
    "\n",
    "        total_loss += orth_loss.item()\n",
    "\n",
    "avg_orth = total_loss / batch_number\n",
    "1 - avg_orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['descriptor_fname'] = './descriptors/my_sun397.json'\n",
    "label_to_classname = label_to_classname_sun397\n",
    "gpt_descriptions, unmodify_dict = load_gpt_descriptions(hparams, label_to_classname)\n",
    "\n",
    "dataset_sun397_trn, dataset_sun397_tst = torch.utils.data.random_split(dataset_sun397, [100000, 8754])\n",
    "dataloader_trn = DataLoader(dataset_sun397_trn, batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "dataloader_val = DataLoader(dataset_sun397_tst, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for batch_number, batch in enumerate(tqdm(dataloader_val)):\n",
    "        images, labels = batch\n",
    "\n",
    "        texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "        tokenized_concepts_list = []\n",
    "        for i in range(len(texts)):\n",
    "            concepts = gpt_descriptions[texts[i]][:5]\n",
    "            tokenized_concepts = tokenizer(concepts)\n",
    "            tokenized_concepts_list.append(tokenized_concepts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        prs.reinit()\n",
    "        representation = model.encode_image(\n",
    "            images, attn_method=\"head\", normalize=False\n",
    "        )\n",
    "        attentions = prs.finalize(representation)\n",
    "\n",
    "        tokenized_concepts_list = torch.stack(tokenized_concepts_list).reshape(-1, 77)\n",
    "        node_text_embeddings = model.encode_text(tokenized_concepts_list.to(device))\n",
    "        node_text_embeddings = node_text_embeddings.reshape(len(images), -1, 512)\n",
    "        \n",
    "        attentions_maps = []\n",
    "        for i in range(len(attentions)):\n",
    "            attentions_map = attentions[i, :, 1:, :].sum(axis=(0, 2)) @ node_text_embeddings[i].T\n",
    "            attentions_maps.append(attentions_map.permute(1, 0))\n",
    "        attentions_maps = torch.stack(attentions_maps)\n",
    "\n",
    "        orth_loss = loss_orth(attentions_maps)\n",
    "\n",
    "        total_loss += orth_loss.item()\n",
    "\n",
    "avg_orth = total_loss / batch_number\n",
    "1 - avg_orth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams['descriptor_fname'] = './descriptors/my_stanfordcars.json'\n",
    "label_to_classname = label_to_classname_stanfordcars\n",
    "gpt_descriptions, unmodify_dict = load_gpt_descriptions(hparams, label_to_classname)\n",
    "\n",
    "dataset_stanfordcars_trn, dataset_stanfordcars_tst = torch.utils.data.random_split(dataset_stanfordcars, [6000, 2144])\n",
    "dataloader_val = DataLoader(dataset_stanfordcars_tst, batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "dataloader_trn = DataLoader(dataset_stanfordcars_trn, batch_size, shuffle=True, num_workers=16, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0.0\n",
    "    for batch_number, batch in enumerate(tqdm(dataloader_val)):\n",
    "        images, labels = batch\n",
    "\n",
    "        texts = np.array(label_to_classname)[labels].tolist()\n",
    "\n",
    "        tokenized_concepts_list = []\n",
    "        for i in range(len(texts)):\n",
    "            concepts = gpt_descriptions[texts[i]][:5]\n",
    "            tokenized_concepts = tokenizer(concepts)\n",
    "            tokenized_concepts_list.append(tokenized_concepts)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        prs.reinit()\n",
    "        representation = model.encode_image(\n",
    "            images, attn_method=\"head\", normalize=False\n",
    "        )\n",
    "        attentions = prs.finalize(representation)\n",
    "\n",
    "        tokenized_concepts_list = torch.stack(tokenized_concepts_list).reshape(-1, 77)\n",
    "        node_text_embeddings = model.encode_text(tokenized_concepts_list.to(device))\n",
    "        node_text_embeddings = node_text_embeddings.reshape(len(images), -1, 512)\n",
    "        \n",
    "        attentions_maps = []\n",
    "        for i in range(len(attentions)):\n",
    "            attentions_map = attentions[i, :, 1:, :].sum(axis=(0, 2)) @ node_text_embeddings[i].T\n",
    "            attentions_maps.append(attentions_map.permute(1, 0))\n",
    "        attentions_maps = torch.stack(attentions_maps)\n",
    "\n",
    "        orth_loss = loss_orth(attentions_maps)\n",
    "\n",
    "        total_loss += orth_loss.item()\n",
    "\n",
    "avg_orth = total_loss / batch_number\n",
    "1 - avg_orth"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
